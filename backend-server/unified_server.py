#!/usr/bin/env python3
"""
çµ±ä¸€çš„å¾Œç«¯æœå‹™å™¨
æ•´åˆ Chrome Extension æ•¸æ“šæ¥æ”¶ã€LLM æœå‹™äº¤äº’å’Œ MCP å·¥å…·åŠŸèƒ½
"""

import asyncio
import json
import sys
import requests
import logging
import io
from datetime import datetime
from typing import Dict, List, Optional, Any
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor

# è¨­ç½® UTF-8 ç·¨ç¢¼
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MCPClient:
    """MCP å®¢æˆ¶ç«¯ï¼Œç”¨æ–¼èˆ‡ MCP æœå‹™å™¨é€šä¿¡"""
    
    def __init__(self):
        self.process = None
        self.tools = []
        self.executor = ThreadPoolExecutor(max_workers=2)
    
    async def start_server(self, command: str, args: List[str]):
        """å•Ÿå‹• MCP æœå‹™å™¨"""
        try:
            self.process = await asyncio.create_subprocess_exec(
                command, *args,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            logger.info(f"MCP server started: {command} {' '.join(args)}")
            
            # åˆå§‹åŒ–æœå‹™å™¨
            await self.initialize()
            
        except Exception as e:
            logger.error(f"Failed to start MCP server: {e}")
            raise
    
    async def send_request(self, method: str, params: Optional[Dict] = None, request_id: str = "1") -> Dict:
        """ç™¼é€è«‹æ±‚åˆ° MCP æœå‹™å™¨"""
        request = {
            "jsonrpc": "2.0",
            "id": request_id,
            "method": method
        }
        
        if params:
            request["params"] = params
        
        request_json = json.dumps(request, ensure_ascii=True)
        
        # ç™¼é€è«‹æ±‚
        if self.process and self.process.stdin:
            self.process.stdin.write((request_json + "\n").encode('utf-8'))
            await self.process.stdin.drain()
            
            # è®€å–éŸ¿æ‡‰
            response_line = await self.process.stdout.readline()
            response_json = response_line.decode('utf-8', errors='replace').strip()
            
            return json.loads(response_json)
        else:
            raise Exception("MCP server not available")
    
    async def initialize(self):
        """åˆå§‹åŒ– MCP æœå‹™å™¨"""
        try:
            response = await self.send_request(
                "initialize",
                {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {
                        "roots": {
                            "listChanged": True
                        }
                    },
                    "clientInfo": {
                        "name": "unified-server-client",
                        "version": "1.0.0"
                    }
                }
            )
            
            if "error" in response:
                raise Exception(f"Initialization failed: {response['error']}")
            
            logger.info("MCP server initialized successfully")
            
            # ç²å–å¯ç”¨å·¥å…·
            await self.list_tools()
        except Exception as e:
            logger.error(f"MCP initialization error: {e}")
    
    async def list_tools(self):
        """ç²å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        try:
            response = await self.send_request("tools/list")
            
            if "error" in response:
                raise Exception(f"Failed to get tools list: {response['error']}")
            
            self.tools = response["result"]["tools"]
            logger.info(f"Found {len(self.tools)} available tools")
        except Exception as e:
            logger.error(f"Error getting tools list: {e}")
            self.tools = []
    
    async def call_tool(self, name: str, arguments: Dict) -> Dict:
        """èª¿ç”¨ MCP å·¥å…·"""
        try:
            response = await self.send_request(
                "tools/call",
                {
                    "name": name,
                    "arguments": arguments
                }
            )
            
            if "error" in response:
                raise Exception(f"Tool call failed: {response['error']}")
            
            return response["result"]
        except Exception as e:
            logger.error(f"Tool call error: {e}")
            return {"content": [{"type": "text", "text": f"å·¥å…·èª¿ç”¨å¤±æ•—: {str(e)}"}]}
    
    async def close(self):
        """é—œé–‰ MCP æœå‹™å™¨"""
        if self.process:
            self.process.terminate()
            await self.process.wait()
            logger.info("MCP server closed")

class UnifiedServer:
    """çµ±ä¸€çš„å¾Œç«¯æœå‹™å™¨"""
    
    def __init__(self):
        self.app = Flask(__name__)
        CORS(self.app, origins="*")
        self.mcp_client = MCPClient()
        self.tabs_data = None
        self.conversation_history = []
        
        # å¾é…ç½®æ–‡ä»¶è®€å– LLM é…ç½®
        self.load_config()
        
        # è¨­ç½®è·¯ç”±
        self.setup_routes()
    
    def load_config(self):
        """åŠ è¼‰é…ç½®"""
        try:
            with open("agent.json", "r", encoding="utf-8") as f:
                config = json.load(f)
            
            self.llm_model = config["model"]
            self.llm_endpoint = config["endpointUrl"]
            logger.info(f"Loaded LLM config: {self.llm_model} @ {self.llm_endpoint}")
            
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            # é»˜èªé…ç½®
            self.llm_model = "Llama-3.2-1B-Instruct-CPU"
            self.llm_endpoint = "http://localhost:8000/api/v1"
    
    def setup_routes(self):
        """è¨­ç½® API è·¯ç”±"""
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            """å¥åº·æª¢æŸ¥"""
            return jsonify({
                "status": "ok",
                "timestamp": datetime.now().isoformat(),
                "services": {
                    "mcp": len(self.mcp_client.tools) > 0,
                    "llm": self.llm_endpoint is not None
                }
            })
        
        @self.app.route('/api/tabs', methods=['POST'])
        def receive_tabs_data():
            """æ¥æ”¶ Chrome Extension çš„æ¨™ç±¤é æ•¸æ“š"""
            try:
                tabs_data = request.get_json()
                
                if not tabs_data or not isinstance(tabs_data.get('tabs'), list):
                    return jsonify({
                        "success": False,
                        "error": "Invalid tabs data format"
                    }), 400
                
                # ä¿å­˜æ¨™ç±¤é æ•¸æ“š
                self.tabs_data = {
                    **tabs_data,
                    "receivedAt": datetime.now().isoformat()
                }
                
                logger.info(f"Received tabs data: {tabs_data.get('totalTabs', 0)} tabs")
                
                return jsonify({
                    "success": True,
                    "message": "Tabs data received",
                    "receivedTabs": tabs_data.get('totalTabs', 0),
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Error processing tabs data: {e}")
                return jsonify({
                    "success": False,
                    "error": str(e)
                }), 500
        
        @self.app.route('/api/tabs', methods=['GET'])
        def get_tabs_data():
            """ç²å–æœ€æ–°çš„æ¨™ç±¤é æ•¸æ“š"""
            return jsonify({
                "success": True,
                "data": self.tabs_data,
                "timestamp": datetime.now().isoformat()
            })
        
        @self.app.route('/api/chat', methods=['POST'])
        def chat_with_llm():
            """èˆ‡ LLM å°è©±"""
            try:
                data = request.get_json()
                user_message = data.get('message', '')
                
                if not user_message:
                    return jsonify({
                        "success": False,
                        "error": "Message is required"
                    }), 400
                
                # è™•ç†ç”¨æˆ¶æ¶ˆæ¯
                response = asyncio.run(self.process_chat_message(user_message))
                
                return jsonify({
                    "success": True,
                    "response": response,
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Chat error: {e}")
                return jsonify({
                    "success": False,
                    "error": str(e)
                }), 500
        
        @self.app.route('/api/chat/stream', methods=['POST'])
        def chat_stream():
            """æµå¼å°è©± API"""
            try:
                data = request.get_json()
                user_message = data.get('message', '')
                
                if not user_message:
                    return jsonify({
                        "success": False,
                        "error": "Message is required"
                    }), 400
                
                def generate_stream():
                    """ç”Ÿæˆæµå¼éŸ¿æ‡‰"""
                    try:
                        # èª¿ç”¨ LLM ä¸¦é€æ­¥è¿”å›çµæœ
                        for chunk in self.call_llm_stream(user_message):
                            yield f"data: {json.dumps(chunk)}\n\n"
                    except Exception as e:
                        yield f"data: {json.dumps({'error': str(e)})}\n\n"
                    finally:
                        yield "data: [DONE]\n\n"
                
                return Response(
                    generate_stream(),
                    mimetype='text/plain',
                    headers={
                        'Cache-Control': 'no-cache',
                        'Connection': 'keep-alive'
                    }
                )
                
            except Exception as e:
                logger.error(f"Stream chat error: {e}")
                return jsonify({
                    "success": False,
                    "error": str(e)
                }), 500
        
        @self.app.route('/api/tools', methods=['GET'])
        def get_available_tools():
            """ç²å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
            return jsonify({
                "success": True,
                "tools": self.mcp_client.tools,
                "timestamp": datetime.now().isoformat()
            })
    
    async def process_chat_message(self, user_message: str) -> str:
        """è™•ç†èŠå¤©æ¶ˆæ¯"""
        try:
            # æ·»åŠ åˆ°å°è©±æ­·å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })
            
            # æº–å‚™ç³»çµ±æ¶ˆæ¯
            system_message = self.build_system_message(user_message)
            
            # èª¿ç”¨ LLM
            messages = [system_message] + self.conversation_history[-10:]  # ä¿æŒæœ€è¿‘10æ¢å°è©±
            llm_response = self.call_llm(messages)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
            tool_result = await self.check_and_use_tools(user_message)
            
            # çµ„åˆæœ€çµ‚éŸ¿æ‡‰
            final_response = llm_response
            if tool_result:
                final_response += f"\n\n{tool_result}"
            
            # æ·»åŠ åŠ©æ‰‹å›æ‡‰åˆ°æ­·å²
            self.conversation_history.append({
                "role": "assistant",
                "content": final_response
            })
            
            return final_response
            
        except Exception as e:
            logger.error(f"Error processing chat message: {e}")
            return f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„æ¶ˆæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def build_system_message(self, user_message: str) -> Dict:
        """æ§‹å»ºç³»çµ±æ¶ˆæ¯"""
        context = ""
        
        # æ·»åŠ æ¨™ç±¤é ä¿¡æ¯
        if self.tabs_data and self.tabs_data.get('tabs'):
            active_tab = next((tab for tab in self.tabs_data['tabs'] if tab.get('isActive')), None)
            if active_tab:
                context += f"ç•¶å‰æ´»å‹•æ¨™ç±¤é : {active_tab.get('title', 'Unknown')}\n"
                context += f"URL: {active_tab.get('url', 'Unknown')}\n"
            context += f"ç¸½å…±æœ‰ {len(self.tabs_data['tabs'])} å€‹æ¨™ç±¤é \n\n"
        
        # æ·»åŠ å¯ç”¨å·¥å…·ä¿¡æ¯
        if self.mcp_client.tools:
            tools_info = []
            for tool in self.mcp_client.tools:
                tools_info.append(f"- {tool['name']}: {tool['description']}")
            context += f"å¯ç”¨å·¥å…·:\n" + "\n".join(tools_info) + "\n\n"
        
        return {
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€å€‹å‹å–„çš„æ¡Œé¢åŠ©æ‰‹ï¼Œå¯ä»¥å¹«åŠ©ç”¨æˆ¶ç®¡ç†ç€è¦½å™¨æ¨™ç±¤é å’Œé€²è¡Œè¨ˆç®—ã€‚
è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡æ½”å›ç­”ï¼Œä¸è¶…é30å€‹å­—ã€‚

ç•¶å‰ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ç”¨æˆ¶è¼¸å…¥: {user_message}"""
        }
    
    def call_llm(self, messages: List[Dict]) -> str:
        """èª¿ç”¨ LLM API"""
        try:
            payload = {
                "model": self.llm_model,
                "messages": messages,
                "max_tokens": 100,
                "temperature": 0.7
            }
            
            response = requests.post(
                f"{self.llm_endpoint}/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            response.raise_for_status()
            result = response.json()
            
            return result["choices"][0]["message"]["content"].strip()
            
        except Exception as e:
            logger.error(f"LLM API call failed: {e}")
            return "æŠ±æ­‰ï¼ŒLLM æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ ğŸ˜…"
    
    def call_llm_stream(self, user_message: str):
        """æµå¼èª¿ç”¨ LLMï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ç°¡åŒ–çš„æµå¼å¯¦ç¾
            response = self.call_llm([{
                "role": "user",
                "content": user_message
            }])
            
            # æ¨¡æ“¬æµå¼è¿”å›
            words = response.split()
            for i, word in enumerate(words):
                yield {
                    "content": word + (" " if i < len(words) - 1 else ""),
                    "fullResponse": " ".join(words[:i+1])
                }
                
        except Exception as e:
            yield {"error": str(e)}
    
    async def check_and_use_tools(self, user_message: str) -> str:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ä¸¦åŸ·è¡Œ"""
        try:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­¸é‹ç®—é—œéµå­—
            math_keywords = ["è¨ˆç®—", "åŠ æ³•", "æ¸›æ³•", "ä¹˜æ³•", "é™¤æ³•", "+", "-", "*", "/", "åŠ ", "æ¸›", "ä¹˜", "é™¤"]
            
            if any(keyword in user_message for keyword in math_keywords):
                # å˜—è©¦æå–æ•¸å­—
                import re
                numbers = re.findall(r'\d+(?:\.\d+)?', user_message)
                
                if len(numbers) >= 2:
                    a = float(numbers[0])
                    b = float(numbers[1])
                    
                    # æ ¹æ“šé—œéµå­—æ±ºå®šé‹ç®—é¡å‹
                    if "åŠ " in user_message or "+" in user_message:
                        result = await self.mcp_client.call_tool("add", {"a": a, "b": b})
                    elif "æ¸›" in user_message or "-" in user_message:
                        result = await self.mcp_client.call_tool("subtract", {"a": a, "b": b})
                    elif "ä¹˜" in user_message or "*" in user_message:
                        result = await self.mcp_client.call_tool("multiply", {"a": a, "b": b})
                    elif "é™¤" in user_message or "/" in user_message:
                        result = await self.mcp_client.call_tool("divide", {"a": a, "b": b})
                    else:
                        result = await self.mcp_client.call_tool("add", {"a": a, "b": b})
                    
                    if result and "content" in result:
                        return result["content"][0]["text"]
            
            return ""
            
        except Exception as e:
            logger.error(f"Tool usage error: {e}")
            return f"å·¥å…·èª¿ç”¨éŒ¯èª¤: {str(e)}"
    
    async def start_mcp_server(self):
        """å•Ÿå‹• MCP æœå‹™å™¨"""
        try:
            await self.mcp_client.start_server("python", ["mcp_server.py"])
            logger.info("MCP server started successfully")
        except Exception as e:
            logger.error(f"Failed to start MCP server: {e}")
    
    def run(self, host="localhost", port=3000, debug=False):
        """é‹è¡Œæœå‹™å™¨"""
        logger.info(f"Starting Unified Server on {host}:{port}")
        
        # åœ¨å¾Œå°å•Ÿå‹• MCP æœå‹™å™¨
        def start_mcp():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self.start_mcp_server())
        
        mcp_thread = threading.Thread(target=start_mcp, daemon=True)
        mcp_thread.start()
        
        # ç­‰å¾… MCP æœå‹™å™¨å•Ÿå‹•
        time.sleep(2)
        
        # å•Ÿå‹• Flask æœå‹™å™¨
        self.app.run(host=host, port=port, debug=debug, threaded=True)
    
    async def close(self):
        """é—œé–‰æœå‹™å™¨"""
        await self.mcp_client.close()

def main():
    """ä¸»å‡½æ•¸"""
    server = UnifiedServer()
    
    try:
        server.run(host="localhost", port=3000, debug=False)
    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
        asyncio.run(server.close())
    except Exception as e:
        logger.error(f"Server error: {e}")
        asyncio.run(server.close())

if __name__ == "__main__":
    main()