{
    "model": "Llama-3.2-1B-Instruct-CPU",
    "endpointUrl": "http://localhost:8000/api/v1",
    "server": {
        "host": "localhost",
        "port": 3000,
        "debug": false
    },
    "mcp": {
        "command": "python",
        "args": ["mcp_server.py"],
        "timeout": 30
    },
    "chat": {
        "max_tokens": 100,
        "temperature": 0.7,
        "max_history": 10
    },
    "logging": {
        "level": "INFO",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    }
}