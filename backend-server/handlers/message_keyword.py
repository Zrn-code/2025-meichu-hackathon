import time
import json
import re
from openai import OpenAI

# Initialize lemonade client (referencing lem.py)
client = OpenAI(
    base_url="http://localhost:8000/api/v1",
    api_key="lemonade"  # éœ€è¦å¸¶ä½†ä¸é©—è­‰
)

# Pattern to remove think blocks (from lem.py)
THINK_BLOCK = re.compile(r"<\s*think\b[^>]*>.*?<\s*/\s*think\s*>",
                         flags=re.IGNORECASE | re.DOTALL)

# ========= 2) æª”æ¡ˆè·¯å¾‘ =========
import os
import glob

# åŸºç¤è·¯å¾‘
BASE_DIR = os.path.join(os.path.dirname(__file__), "..", "..", "windows-app", "src", "data")
VIDEO_SUBTITLES_DIR = os.path.join(BASE_DIR, "video_subtitles")
CHARACTER_DIR = os.path.join(BASE_DIR, "character") 
AVATAR_TALK_DIR = os.path.join(BASE_DIR, "avatar_talk")

# è§’è‰²æª”æ¡ˆ
CH_JSON = os.path.join(CHARACTER_DIR, "1.json")   

# ========= 3) å…¨åŸŸè®Šæ•¸å’Œåˆå§‹åŒ– =========
def _load_character_data():
    """è¼‰å…¥è§’è‰²è³‡æ–™"""
    with open(CH_JSON, "r", encoding="utf-8") as f:
        return json.load(f)

def _ensure_output_dir():
    """ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨"""
    os.makedirs(AVATAR_TALK_DIR, exist_ok=True)


# ========= 4) è™•ç†å‡½æ•¸ =========
def build_keyword(captions: str) -> str:
    return (
        "Role: Professional-term extractor.\n"
        "Task: Return EXACTLY ONE TOKEN that is a domain-specific professional term.\n"
        "If no valid professional term exists, return exactly: null\n"
        "Hard rules:\n"
        "- Output must be ONE token only (no spaces, no punctuation, no quotes).\n"
        "- Generic/common words are forbidden; if only generic words exist, return null.\n"
        "Do NOT explain. Output ONLY the final token or null.\n"
        f"Sentence: {captions}\n"
        "Keyword:"
    )

def normalize_kw(s: str) -> str:
    s = s.strip()
    # ç§»é™¤å¤–åœå¼•è™Ÿ/å…¨å½¢ç©ºç™½
    s = s.strip('"\u3000 ')
    # å¤§å°å¯«ä¸€å¾‹å°å¯«ï¼ˆè‹±æ•¸é¡ï¼‰
    s = s.lower()
    return 

SPLIT_PATTERN = re.compile(r"[ã€,\uFF0C;/\|\n\r]+") 

def process_video_subtitles(video_data, character_info, video_id=None):
    """è™•ç†å–®å€‹å½±ç‰‡å­—å¹•æª”æ¡ˆ"""
    llm_summary = []
    seen_keywords = set()
    stamp = ""
    reply_count = 0
    
    transcript_data = video_data.get("transcript_data", {})
    content = transcript_data.get("content", [])
    
    print(f"ğŸ¬ é–‹å§‹è™•ç†å½±ç‰‡å­—å¹•ï¼Œå…± {len(content)} å€‹å­—å¹•æ®µè½")

    for s in content:
        if index >= 100:
            break  # æ¸¬è©¦æ™‚å…ˆåªè·‘ 100 å€‹æ®µè½
        text = s.get("text", "")
        stamp = s.get("offset", "")
        prompt  = build_keyword(text)
        ban = ["\n", ",", ".", "/", "(", "[", "domain", "professional", "term", "specific", "none"]
    
        resp = client.chat.completions.create(
            model="user.Roleplay-Llama-3-8B-i1-GGUF",
            messages=[{"role": "user", "content": prompt}],
            stream=False,
            max_tokens=32,
            temperature=0.8,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=ban
        )
        content_reply = resp.choices[0].message.content
        reply_text = THINK_BLOCK.sub("", content_reply).strip()

        def contains_banned_substring(norm: str, ban: list[str]) -> bool:
            s = norm.lower()
            return any(b.lower() in s for b in ban)
        
        norm = normalize_kw(reply_text)
        if not norm:
            continue
        if contains_banned_substring(norm, seen_keywords):
            continue  # ğŸ‘ˆ å·²å‡ºç¾éï¼Œä¸åŠ å…¥
        if contains_banned_substring(norm, ban):
            continue  # åœç”¨è©ä¹Ÿä¸åŠ å…¥
        if norm == "null":
            continue  # null ä¹Ÿä¸åŠ å…¥
        index += 1
        print(f"ğŸ’­ æ­£åœ¨ç”Ÿæˆç¬¬ {index} å€‹é—œéµå­—:{norm}")
        seen_keywords.add(norm)

        # ä¿ç•™åŸå­—é¢ï¼ˆæœªæ­£è¦åŒ–ï¼‰æ”¾åˆ°è¼¸å‡º
        llm_summary.append({
            "time": stamp,
            "Keyword": norm
        })
    
    print(f"ğŸ“ å®Œæˆè™•ç†ï¼Œç¸½å…±ç”Ÿæˆäº† {reply_count} å€‹è§’è‰²å›å¾©")
    return llm_summary

def generate_avatar_response_for_video(video_id, character_index=0):
    """ç‚ºæŒ‡å®šå½±ç‰‡ ID ç”Ÿæˆè§’è‰²å›æ‡‰
    
    Args:
        video_id (str): å½±ç‰‡ ID
        character_index (int): è§’è‰²ç´¢å¼•ï¼Œé è¨­ç‚º 0 (ç¬¬ä¸€ä½è§’è‰²)
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
    """
    try:
        import logging
        logger = logging.getLogger(__name__)
        
        print(f"\nğŸ¯ é–‹å§‹è™•ç†å½±ç‰‡: {video_id}")
        print(f"ğŸ“ å­—å¹•æª”æ¡ˆä¾†æº: video_subtitles/{video_id}.json")
        print("="*50)
        
        # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
        _ensure_output_dir()
        
        # è¼‰å…¥è§’è‰²è³‡æ–™
        character_info = _load_character_data()
        character_name = character_info[0] if character_info else "é è¨­è§’è‰²"
        print(f"ğŸ‘¤ ä½¿ç”¨è§’è‰²: {character_name}")
        
        # æ§‹å»ºè¼¸å…¥æª”æ¡ˆè·¯å¾‘
        subtitle_file = os.path.join(VIDEO_SUBTITLES_DIR, f"{video_id}.json")
        
        if not os.path.exists(subtitle_file):
            print(f"âŒ å­—å¹•æª”æ¡ˆä¸å­˜åœ¨: {subtitle_file}")
            logger.warning(f"å­—å¹•æª”æ¡ˆä¸å­˜åœ¨: {subtitle_file}")
            return False
        
        # æª¢æŸ¥è§’è‰²å›æ‡‰æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        output_file = os.path.join(AVATAR_TALK_DIR, f"{video_id}.json")
        if os.path.exists(output_file):
            print(f"âš ï¸  è§’è‰²å›æ‡‰æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éè™•ç†: avatar_talk/{video_id}.json")
            logger.info(f"è§’è‰²å›æ‡‰æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éè™•ç†: {output_file}")
            return True
        
        logger.info(f"é–‹å§‹ç‚ºå½±ç‰‡ {video_id} ç”Ÿæˆè§’è‰²å›æ‡‰...")
        
        # è®€å–å½±ç‰‡å­—å¹•è³‡æ–™
        with open(subtitle_file, "r", encoding="utf-8") as f:
            video_data = json.load(f)
        
        print(f"ğŸ“– æˆåŠŸè¼‰å…¥å­—å¹•æª”æ¡ˆ")
        
        # è™•ç†å­—å¹•ç”Ÿæˆå›æ‡‰
        llm_summary = process_video_subtitles(video_data, character_info, video_id)
        
        # å„²å­˜çµæœ
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(llm_summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ°: avatar_talk/{video_id}.json")
        print("="*50)
        print(f"âœ… å½±ç‰‡ {video_id} è™•ç†å®Œæˆï¼\n")
        
        logger.info(f"è§’è‰²å›æ‡‰å·²ç”Ÿæˆä¸¦å„²å­˜åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ è™•ç†å½±ç‰‡ {video_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.error(f"ç”Ÿæˆè§’è‰²å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def generate_avatar_responses_batch():
    """æ‰¹æ¬¡è™•ç†æ‰€æœ‰å½±ç‰‡å­—å¹•æª”æ¡ˆç”Ÿæˆè§’è‰²å›æ‡‰"""
    import logging
    logger = logging.getLogger(__name__)
    
    start_time = time.time()
    
    print("\nğŸ”„ é–‹å§‹æ‰¹æ¬¡è™•ç†æ‰€æœ‰å½±ç‰‡å­—å¹•æª”æ¡ˆ")
    print("="*60)
    
    # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
    _ensure_output_dir()
    
    # è¼‰å…¥è§’è‰²è³‡æ–™
    character_info = _load_character_data()
    
    # å–å¾—æ‰€æœ‰å½±ç‰‡å­—å¹•æª”æ¡ˆ
    subtitle_files = glob.glob(os.path.join(VIDEO_SUBTITLES_DIR, "*.json"))
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(subtitle_files)} å€‹å½±ç‰‡å­—å¹•æª”æ¡ˆ")
    logger.info(f"æ‰¾åˆ° {len(subtitle_files)} å€‹å½±ç‰‡å­—å¹•æª”æ¡ˆ")
    
    success_count = 0
    
    for subtitle_file in subtitle_files:
        try:
            logger.info(f"è™•ç†æª”æ¡ˆ: {subtitle_file}")
            
            # å–å¾—æª”æ¡ˆåç¨±ï¼ˆä¸åŒ…å«å‰¯æª”åï¼‰ä½œç‚º video ID
            video_id = os.path.splitext(os.path.basename(subtitle_file))[0]
            
            # æª¢æŸ¥è§’è‰²å›æ‡‰æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
            output_file = os.path.join(AVATAR_TALK_DIR, f"{video_id}.json")
            if os.path.exists(output_file):
                print(f"âš ï¸  å½±ç‰‡ {video_id} çš„è§’è‰²å›æ‡‰æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éè™•ç†")
                logger.info(f"è§’è‰²å›æ‡‰æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éè™•ç†: {output_file}")
                success_count += 1  # è¦–ç‚ºæˆåŠŸï¼Œå› ç‚ºæª”æ¡ˆå·²å­˜åœ¨
                continue
            
            # è®€å–å½±ç‰‡å­—å¹•è³‡æ–™
            with open(subtitle_file, "r", encoding="utf-8") as f:
                video_data = json.load(f)
            
            # è™•ç†é€™å€‹å½±ç‰‡çš„å­—å¹•
            print(f"\nğŸ¯ è™•ç†å½±ç‰‡: {video_id}")
            llm_summary = process_video_subtitles(video_data, character_info, video_id)
            
            # å„²å­˜çµæœ
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(llm_summary, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å·²å„²å­˜åˆ°: avatar_talk/{video_id}.json")
            logger.info(f"å·²å„²å­˜åˆ°: {output_file}")
            success_count += 1
            
        except Exception as e:
            print(f"âŒ è™•ç†æª”æ¡ˆ {subtitle_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"è™•ç†æª”æ¡ˆ {subtitle_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    elapsed = time.time() - start_time
    print("="*60)
    print(f"ğŸ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    print(f"âœ… æˆåŠŸè™•ç†: {success_count}/{len(subtitle_files)} å€‹æª”æ¡ˆ")
    print(f"â±ï¸  ç¸½æ™‚é–“: {elapsed:.2f} ç§’")
    print("="*60)
    
    logger.info(f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {success_count}/{len(subtitle_files)} å€‹æª”æ¡ˆï¼Œç¸½æ™‚é–“: {elapsed:.2f} ç§’")
    
    return success_count == len(subtitle_files)

# ========= 5) ä¸»ç¨‹å¼éƒ¨åˆ† =========
if __name__ == "__main__":
    # ç•¶ä½œç‚ºç¨ç«‹è…³æœ¬åŸ·è¡Œæ™‚
    import sys
    
    if len(sys.argv) > 1:
        # å¦‚æœæä¾›äº† video ID åƒæ•¸ï¼Œåªè™•ç†è©²å½±ç‰‡
        video_id = sys.argv[1]
        print(f"è™•ç†æŒ‡å®šå½±ç‰‡: {video_id}")
        success = generate_avatar_response_for_video(video_id)
        if success:
            print(f"å½±ç‰‡ {video_id} çš„è§’è‰²å›æ‡‰ç”Ÿæˆå®Œæˆ")
        else:
            print(f"å½±ç‰‡ {video_id} çš„è§’è‰²å›æ‡‰ç”Ÿæˆå¤±æ•—")
    else:
        # å¦å‰‡æ‰¹æ¬¡è™•ç†æ‰€æœ‰å½±ç‰‡
        print("é–‹å§‹æ‰¹æ¬¡è™•ç†æ‰€æœ‰å½±ç‰‡...")
        success = generate_avatar_responses_batch()
        if success:
            print("æ‰€æœ‰å½±ç‰‡è™•ç†å®Œæˆ")
        else:
            print("éƒ¨åˆ†å½±ç‰‡è™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")